{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89403cbd",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d337395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate scipy tensorboardX peft bitsandbytes transformers trl tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f8349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\hyunwook\\anaconda3\\envs\\llm_practice\\lib\\site-packages (1.1.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c27bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyunwook\\anaconda3\\envs\\snu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, # 인과적 언어 추론(예: GPT)을 위한 모델을 자동으로 불러오는 클래스\n",
    "    AutoTokenizer,        # 입력 문장을 토큰 단위로 자동으로 잘라주는 역할\n",
    "    BitsAndBytesConfig,   # 모델 구성 (벡터의 INT8 최대 절대값 양자화 기법을 사용할 수 있도록 도와주는 Meta AI의 라이브러리)\n",
    "    HfArgumentParser,     # 파라미터 파싱\n",
    "    TrainingArguments,    # 훈련 설정\n",
    "    pipeline,             # 파이프라인 설정 \n",
    "    logging,              # 로깅을 위한 클래스\n",
    ")\n",
    "\n",
    "# 모델 튜닝을 위한 라이브러리\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2b57d",
   "metadata": {},
   "source": [
    "### Llama 2 모델과 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246ed3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face 허브에서 훈련하고자 하는 모델을 가져와서 이름 지정\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# instruction 데이터 세트 설정\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# fine-tuning을 거친 후의 모델에 부여될 새로운 이름을 지정\n",
    "new_model = \"tuned-llama-2-7b-miniguanaco\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fa974c",
   "metadata": {},
   "source": [
    "### LoRA(Low-Rank Adaptation) 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a74177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA에서 사용하는 low-rank matrices 어텐션 차원을 정의. 여기서는 64로 설정\n",
    "# 값이 크면 클수록 더 많은 수정이 이루어지며, 모델이 더 복잡해질 수 있음\n",
    "lora_r = 16 # sionic ai에선 64\n",
    "# 값이 작을수록 학습 시간, 리소스 사용량, 학습할 수 있는 표현의 다양성(복잡성) 감소\n",
    "# 클수록 증가\n",
    "# 쉽게 말해, 클수록 성능이 좋아지고 메모리 사용량도 증가\n",
    "# 값이 크면 과적합, 작으면 과소적합\n",
    "# 기존 모델의 파라미터에 영향을 주는 레이어가 저차원 행렬인듯한데\n",
    "# 저차원 행렬의 차원을 결정한다.\n",
    "\n",
    "# LoRA 적용 시 가중치에 곱해지는 스케일링 요소. 여기서는 16으로 설정\n",
    "# LoRA가 적용될 때 원래 모델의 가중치에 얼마나 영향을 미칠지 결정. 높은 값은 가중치 조정의 강도를 증가시킴\n",
    "lora_alpha = 16\n",
    "# 값이 낮으면 기존 데이터를 위주로 높으면 새로운 데이터 위주로 훈련 데이터를 수용\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "# LoRA 층에 적용되는 드롭아웃 확률. 여기서는 0.1 (10%)로 설정\n",
    "lora_dropout = 0.05 # 일부 네트워크 연결을 무작위로 비활성화하여 모델의 강건함에 기여\n",
    "\n",
    "# dropout은 랜덤하게 뉴런을 비활성화하여 계산량을 줄이는 것으로 자원을 효율적으로 사용하는 반면\n",
    "# lora에서는 랜덤하게 뉴런을 비활성화 하면서 이후의 레이어에 미치는 영향을 줄이는 방법으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4cf82",
   "metadata": {},
   "source": [
    "### `bitsandbytes` 파라미터 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3277af",
   "metadata": {},
   "source": [
    "- `bitsandbytes`는 QLoRA기법을 적용하기 위해 사용되는 8비트 양자회 라이브러리\n",
    "- 양자화 관련 설정값을 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d40c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit precision 기반의 모델 로드\n",
    "use_4bit = True \n",
    "\n",
    "# 4비트 기반 모델에 대한 dtype 계산\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# 양자화 유형(fp4 또는 nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 4비트 기 모델에 대해 중첩 양자화 활성화(이중 양자화)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9caf22",
   "metadata": {},
   "source": [
    "### `TrainingArguments`파라미터 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506630f",
   "metadata": {},
   "source": [
    "- 허깅페이스에서 제공하는 라이브러리 모델\n",
    "- 학습부터 평가까지 한번에 해결할 수 있는 API 제공\n",
    "- Optimizer의 종류, Learning Rate, Epoch 수, Scheduler와 Half Precicison의 사용 여부 등을 지정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ef3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델이 예측한 결과와 체크포인트가 저장될 출력 디렉터리\n",
    "output_dir = \"./results\" \n",
    "\n",
    "# 훈련 에포크 수\n",
    "num_train_epochs = 1 \n",
    "\n",
    "# fp16/bf16 학습 활성화(A100으로 bf16을 True로 설정)\n",
    "fp16 = False   \n",
    "bf16 = True\n",
    "\n",
    "# 각 GPU별 훈련용 배치 크기\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# 각 GPU별 평가용 배치 크기\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# 기울기 갱신 전 업데이트 축적 횟수\n",
    "gradient_accumulation_steps = 8\n",
    "# 2이상으로 설정 시 기존의 업데이트 내역은 사라지는지\n",
    "# (기울기 업데이트가 항상 좋은 쪽으로만 업데이트 되진 않는 것을 알고 있기에)\n",
    "\n",
    "# 그래디언트 체크포인트 활성화\n",
    "gradient_checkpointing = True  \n",
    "# 필요할 때만 특정 계층의 기울기를 저장하고 나머지는 버려 메모리의 부담을 줄인다.\n",
    "\n",
    "# 그래디언트 클리핑을 위한 최대 그래디언트 노름을 설정. \n",
    "# 그래디언트 클리핑은 그래디언트의 크기를 제한하여 훈련 중 안정성을 높임.\n",
    "# Maximum gradient normal (그래디언트 클리핑) 0.3으로 설정\n",
    "max_grad_norm = 0.3\n",
    "# 모델이 데이터로부터 학습하는 속도를 조절\n",
    "# 기울기가 과도하게 커져 gradient exploding 문제를 방지\n",
    "# 기울기 소실과 반대되는 것으로 역전파를 수행할 때 기울기의 크기가 점점 커져 발산하는 현상\n",
    "\n",
    "# 초기 학습률 AdamW 옵티마이저\n",
    "learning_rate = 2e-6\n",
    "# 학습률을 낮게 잡아 학습하는 속도를 적절히 늦췄다.\n",
    "\n",
    "# bias/LayerNorm 가중치를 제외하고 모든 레이어에 적용할 Weight decay 값\n",
    "weight_decay = 0.001\n",
    "# 모델의 가중치가 너무 큰 값을 가지지 않도록 함\n",
    "# 이로 오버피팅 현상을 해소 가능하다.\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optim = \"paged_adamw_32bit\"  \n",
    "\n",
    "# 학습률 스케줄러의 유형 설정, 여기서는 코사인 스케줄러 사용\n",
    "lr_scheduler_type = \"cosine\"\n",
    "# 안정적으로 끊임없이 Loss가 감소하게 한다고 함\n",
    "# 어떻게?\n",
    "\n",
    "# 훈련 스텝 수(num_train_epochs 재정의)\n",
    "max_steps = -1 # epoch 기준으로 한 번만 전체 데이터를 돈다는 뜻\n",
    "\n",
    "# (0부터 learning rate까지) 학습 초기에 학습률을 점진적으로 증가시키 linear warmup 스텝의 Ratio\n",
    "warmup_ratio = 0.03  \n",
    "\n",
    "# 시퀀스를 동일한 길이의 배치로 그룹화, 메모리 절약 및 훈련 속도를 높임\n",
    "group_by_length = True   \n",
    "\n",
    "# X 업데이트 단계마다 체크포인트 저장\n",
    "save_steps = 0\n",
    "\n",
    "# 매 X 업데이트 스텝 로그\n",
    "logging_steps = 25  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684e7c9",
   "metadata": {},
   "source": [
    "### SFT 파라미터 값 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af75c17",
   "metadata": {},
   "source": [
    "- 프롬프트 데이터셋을 이용하여 베이스 모델을 지도학습 바탕으로 파인 튜닝하는 기법으로, 일정 토큰에 대한 다음 토큰을 예측하는 형식으로 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f59165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 입력 시퀀스 길이 설정\n",
    "max_seq_length = 512\n",
    "# 1024로 진행 시 GPU 메모리 부족(GeForce 4060 NoteBook)\n",
    "\n",
    "# 하나의 입력 시퀀스에 여러 개의 짧은 예시 문장을 한번에 넣어 GPU효율성을 높일 수 있음\n",
    "packing = False\n",
    "\n",
    "# GPU를 몇 번 로드할 지 지정\n",
    "# GPU가 하나기에 0으로 설정 (= 전체 모델 로드)\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369c4cf",
   "metadata": {},
   "source": [
    "### 데이터 세트 로딩과 데이터 타입 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f36e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4423a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be151d",
   "metadata": {},
   "source": [
    "### csv or excel파일을 데이터셋으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd46718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # CSV 파일 → Dataset\n",
    "# dataset_csv = load_dataset(\"csv\", data_files=\"mydata.csv\")\n",
    "\n",
    "# # Excel 파일 → Dataset\n",
    "# dataset_xlsx = load_dataset(\"excel\", data_files=\"mydata.xlsx\")\n",
    "\n",
    "# print(dataset_csv[\"train\"])\n",
    "# print(dataset_xlsx[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67ae9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# 모델 계산에 사용될 데이터 타입 결정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit, # 모델을 4비트로 로드할지 여부\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type, # 양자화 유형 설정\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # 계산에 사용될 데이터 타입 설정\n",
    "    bnb_4bit_use_double_quant=use_nested_quant # 중첩 양자화를 사용할지 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cf6de",
   "metadata": {},
   "source": [
    "### GPU 호환성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea14d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 만약 GPU가 최소한 버전 8 이상이라면 (major >= 8) bfloat16을 지원한다고 메시지를 출력. \n",
    "# bfloat16은 훈련 속도를 높일 수 있는 데이터 타입. \n",
    "\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52238eda",
   "metadata": {},
   "source": [
    "### 확인용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7078a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "12.1\n",
      "8801\n",
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)         # 2.1.2+cu118\n",
    "print(torch.version.cuda)        # 11.8\n",
    "print(torch.backends.cudnn.version())  # cudnn 버전\n",
    "print(torch.cuda.is_available()) # True 여야 정상\n",
    "print(torch.cuda.get_device_name(0))  # GPU 모델명 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "813559ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_dtype: torch.bfloat16\n",
      "use_4bit: True\n",
      "cuda available: True\n",
      "capability: (8, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"compute_dtype:\", compute_dtype)\n",
    "print(\"use_4bit:\", use_4bit)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"capability:\", torch.cuda.get_device_capability())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9adf67b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2ae2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa46db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST] Самый великий человек из всех живших на планете? [/INST] Для начала нужно выбрать критерии величия человека. Обычно великим называют человека, который внес большой вклад в общество или сильно выделялся на фоне других в своем деле.\\n\\nНапример, Иосифа Бродского считают великим поэтом, а Иммануила Канта — великим философом. Александр Македонский, известный тем, что собрал в свои владения огромную империю (включавшую Македонию, Грецию, Персию, Египет), в историографии носит имя Александр Великий. Для христиан, скорее всего, самым великим человеком жившим на земле был Иисус Христос, так как он совершил множество благих деяний и совершил подвиг ради человечества. \\n\\nПри этом, когда мы выдвигаем одну личность на роль великого человека, сразу же находится множество людей, не согласных с этим. Того же Иосифа Бродского, хоть он и получил престижную Нобелевскую премию, некоторые люди считают графоманом и посредственным поэтом. \\n\\nВ целом, кого считать великим — это самостоятельный выбор каждого человека, который можно сделать только соотнося его со своими личными ценностями и представлениями о красивом, правильном, хорошем. </s>'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7a1a993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bc6c7",
   "metadata": {},
   "source": [
    "### 베이스 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f0934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]\n",
      "c:\\Users\\hyunwook\\anaconda3\\envs\\llm_practice\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\hyunwook\\anaconda3\\envs\\llm_practice\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyunwook\\anaconda3\\envs\\llm_practice\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyunwook\\anaconda3\\envs\\llm_practice\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1     # 분산학습 시 사용하는 옵션, 1이면 기본(나누지 않음)\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 동일한 batch 내에서 입력의 크기를 동일하기 위해서 사용하는 Padding Token을 End of Sequence라고 하는 Special Token으로 사용한다.\n",
    "tokenizer.pad_token = tokenizer.eos_token   # padding token을 EOS로 설정 Llama2에는 padding token이 없어서 설정\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training. Padding을 오른쪽 위치에 추가한다.\n",
    "# 문장 + Padding(EOS), fp16학습 시 왼쪽에 패딩이 있으면 버그 유발 가능성 있음\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",    # bias 파라미터는 수정하지 않는다.\n",
    "    task_type=\"CAUSAL_LM\", # 파인튜닝할 태스크를 Optional로 지정할 수 있는데, 여기서는 CASUAL_LM을 지정하였다.\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,      # 학습 결과 저장 위치\n",
    "    num_train_epochs=num_train_epochs,  # 학습 반복 횟수\n",
    "    per_device_train_batch_size=per_device_train_batch_size,    # GPU 하나당 batch 크기\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,    # 여러 step에서 grad 쌓아서 효과적으로 큰 batch처럼 학습\n",
    "    optim=optim,    # optimizer 종류\n",
    "    save_steps=save_steps,  # 몇 step마다 체크포인트 저장할지\n",
    "    logging_steps=logging_steps,    # 몇 step마다 로그 출력할지\n",
    "    learning_rate=learning_rate,    # 학습률\n",
    "    weight_decay=weight_decay,  # 규제\n",
    "    fp16=fp16,  # 사용여부\n",
    "    bf16=bf16,  # 이하동문\n",
    "    max_grad_norm=max_grad_norm,    # gradient clipping 최대값\n",
    "    max_steps=max_steps,    # 총 학습 step(epoch대신 step으로 제어 가능)\n",
    "    warmup_ratio=warmup_ratio,  # 학습 초반에 learning rate를 천천히 올리는 비율\n",
    "    group_by_length=group_by_length,    # 비슷한 길이의 샘플끼리 묶어서 효율적으로 학습\n",
    "    lr_scheduler_type=lr_scheduler_type,    # learning rate 스케줄러 종류\n",
    "    report_to=\"tensorboard\" # 로그 기록을 TensorBoard로 보냄\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,    # LoRA 설정\n",
    "    dataset_text_field=\"text\",  # 데이터셋에서 텍스트가 들어있는 열 이름 (최근에는 빼는 추세)\n",
    "    max_seq_length=max_seq_length,  # 토큰 시퀀스 최대 길이(넘어가면 자름)\n",
    "    tokenizer=tokenizer,    # 토크나이저\n",
    "    args=training_arguments,    # 학습 설정 전달\n",
    "    packing=packing,    # 여러 샘플을 이어 붙여 max seq_length를 채워 학습할지 여부(사용 시 효율 상승)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62806da",
   "metadata": {},
   "source": [
    "### 모델 훈련과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb64c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# 훈련이 완료된 모델을 'new_model'에 저장 \n",
    "trainer.model.save_pretrained(new_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e515b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model과 new_model에 저장된 LoRA 가중치를 통합하여 새로운 모델을 생성\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model) # LoRA 가중치를 가져와 기본 모델에 통합\n",
    "# model = PeftModel.from_pretrained(base_model, \"tuned-llama-2-7b-miniguanaco\") # 나중에 저장된 파인튜닝된 레이어를 불러오고싶을때 폴더 이름을 적어줘야한다. .safetensors, .json파일 모두 들어있어야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "\n",
    "# 사전 훈련된 토크나이저를 다시 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  \n",
    "\n",
    "# 토크나이저의 패딩 토큰을 종료 토큰(end-of-sentence token)과 동일하게 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "# 패딩을 시퀀스의 오른쪽에 적용\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b454a",
   "metadata": {},
   "source": [
    "### 출처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd7db6",
   "metadata": {},
   "source": [
    "- LLaMA 공식 허깅페이스: meta-llama/Llama-2-7b · Hugging Face\n",
    "- 튜닝에 사용한 데이터 : https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k\n",
    "- 참고한 사이트: https://blog.sionic.ai/finetuning_llama"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
